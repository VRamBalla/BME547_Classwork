{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VRamBalla/BME547_Classwork/blob/main/Project2_Team5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE: This notebook assumes that the folder \"team_5\" is placed as a shortcut in your drive. "
      ],
      "metadata": {
        "id": "YPnF9UUaWZlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install and Import Modules"
      ],
      "metadata": {
        "id": "p3Ly5nZPP6Aw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVenJIn3J_na",
        "outputId": "0d671403-ea2c-45ad-cdfb-85bff9b12a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: exrex in /usr/local/lib/python3.9/dist-packages (0.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install exrex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dxkyJcYEFG_",
        "outputId": "680b769d-8cf7-4c33-cc95-e75aa419e8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV4iEe5vMXIR",
        "outputId": "aece2133-81df-42fa-da72-fb6d126ea810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import os\n",
        "import sys\n",
        "dataset_path = '/content/gdrive/My Drive/team_5/project_2/'\n",
        "sys.path.append(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZe7acMBI4-a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import pickle\n",
        "import exrex\n",
        "import re\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import layers\n",
        "from keras import models\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRUMBO61JndK"
      },
      "source": [
        "#Fine-Tuning ProtGPT2 on Cas9 Sequences\n",
        "This section of code allows us to pass Cas9 sequence data into ProtGPT2, so that it may change its weights in a directed fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ZHQL3DJuTJ"
      },
      "outputs": [],
      "source": [
        "data_8nt = pd.read_csv(dataset_path + 'Final_Cleaned_Dataset.csv') #This reads in the Cas9 dataset provided by Dr. Chatterjee\n",
        "\n",
        "protein_sequences = data_8nt['AA Sequence'].values.tolist() #Puts all of the amino acid sequences into a list\n",
        "seq_len = list(data_8nt['Sequence Length']) #Creates a list of the lenght of every amino acid sequence\n",
        "max_seq_len = np.max(seq_len) #The maximum amino acid sequence length in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_8nt['AA Sequence'] = data_8nt['AA Sequence'].apply(lambda x: '<|endoftext|>'+x+'<|endoftext|>') #We add the <|endoftext|> string to each end of the protein sequences so that they may be used in the fine-tuning of ProtGPT2\n",
        "\n",
        "train_seq, validate_seq, _, _ = train_test_split(data_8nt['AA Sequence'], np.zeros((data_8nt.shape[0], 1)), train_size=0.8) #Generate training, and validation data for the fine-tuning of ProtGPT2\n",
        "\n",
        "np.savetxt('/content/gdrive/My Drive/team_5/project_2/ProtGPT2/ProtGPT2_train.txt', train_seq.values, fmt='%s') #Saves the training and validation data as txt files\n",
        "np.savetxt('/content/gdrive/My Drive/team_5/project_2/ProtGPT2/ProtGPT2_val.txt', validate_seq.values, fmt='%s')"
      ],
      "metadata": {
        "id": "xdrTO2Ky-PfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUuWl4TBL6Of"
      },
      "outputs": [],
      "source": [
        "!pip install -r '/content/gdrive/My Drive/team_5/project_2/ProtGPT2/requirements.txt' #Installs the necessary requirements for ProtGPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The line below is the training of the ProtGPT2 model. A batch size of 2 was utilized, with a learning rate of 1e-05 using the Adam optimizer"
      ],
      "metadata": {
        "id": "ItnZhCbNAfto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python '/content/gdrive/My Drive/team_5/project_2/ProtGPT2/run_clm.py' --model_name_or_path nferruz/ProtGPT2 --per_device_train_batch_size 2 --train_file '/content/gdrive/My Drive/team_5/project_2/ProtGPT2/ProtGPT2_train.txt' --validation_file '/content/gdrive/My Drive/team_5/project_2/ProtGPT2/ProtGPT2_val.txt' --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir '/content/gdrive/My Drive/team_5/project_2/ProtGPT2/ProtGPT2_output'  --learning_rate 1e-05 --overwrite_output_dir"
      ],
      "metadata": {
        "id": "ooX8ULokAIGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation of New Sequences\n",
        "The following lines of code generate novel sequences from our fine-tuned ProtGPT2 model. We generated 100 sequences.\n",
        "\n",
        "This code was only run once since we only needed to fine-tune and generate sequences from ProtGPT2 one time"
      ],
      "metadata": {
        "id": "OCRIwexSHqKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline \n",
        "protgpt2 = pipeline('text-generation', model= dataset_path + 'ProtGPT2/ProtGPT2_output/') #here, we are initializing the fine-tuned ProtGPT2 model that was created from the previous training step."
      ],
      "metadata": {
        "id": "-zbElVj2Lqgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We generated 100 amino acid sequences from the fine-tuned model. We have put the restriciton that the amino acid sequence must start with M, as this is the start codon amino acid.\n",
        "\n",
        "The final line prints all of the generated sequences; note that every 60 amino acids, there is a \\n entry. This is consistent with the FASTA file format, and is used by ProtGPT2 later in the perplexity calculations"
      ],
      "metadata": {
        "id": "LELOzp3mQZJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3IKSEZ6RqEQ"
      },
      "outputs": [],
      "source": [
        "sequences = protgpt2(\"M\", max_length=max_seq_len, do_sample=True, top_k=950, repetition_penalty=1.2, num_return_sequences=100, eos_token_id=0)\n",
        "for seq in sequences:\n",
        "        print(seq) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGanfr09RvKz"
      },
      "outputs": [],
      "source": [
        "gen_seq=[] #Initialize a list in which the generated sequences will be stored\n",
        "for i in range(0,len(sequences)):\n",
        "  gen_seq.append(sequences[i]['generated_text']) #Append each generated protein sequence to the list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import math\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(dataset_path + 'ProtGPT2/ProtGPT2_output/') #Load in the tokenizer from the fine-tuned ProtGPT2 Model\n",
        "model = GPT2LMHeadModel.from_pretrained(dataset_path + 'ProtGPT2/ProtGPT2_output/') #Load in the stored fine-tuned ProtGPT2 model\n",
        "\n",
        "#Function to calculate perplexity\n",
        "def calculatePerplexity(sequence, model, tokenizer):\n",
        "    input_ids = torch.tensor(tokenizer.encode(sequence)).unsqueeze(0) \n",
        "    input_ids = input_ids\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    loss, logits = outputs[:2]\n",
        "    return math.exp(loss)\n",
        "\n",
        "\n",
        "ppl = [] #Initialize list for storage of perplexity values\n",
        "for i in range(0,len(gen_seq)):\n",
        "  sequence=gen_seq[i]\n",
        "  sequence = '<|endoftext|>' + sequence + '<|endoftext|>' #Add these delimiter strings at the beginning and end of every amino acid sequence\n",
        "  ppl.append(calculatePerplexity(sequence, model, tokenizer)) #Append the perplexity value of each generated sequences to the perplexity list\n"
      ],
      "metadata": {
        "id": "JZ0Be6xccKoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sequences = [] #This list will store the \"clean\" version of the generated sequence; without the \\n separators in the previous FASTA format\n",
        "\n",
        "for i in range(0,len(gen_seq)):\n",
        "  s = gen_seq[i]\n",
        "  n = s.split('\\n') #Split the generated sequence by all instances of \"\\n\"\n",
        "  s_new = ''.join(n) #Join the split parts of the sequence back together. The result is a string that only consists of amino acids\n",
        "  clean_sequences.append(s_new) #Append the clean sequence to the clean_sequences list\n",
        "\n",
        "generated_sequences = pd.DataFrame( #Initialize a dataframe in which the generated amino acid sequences and perplexity values will be stored\n",
        "    {'AA Sequence':clean_sequences,\n",
        "     'Perplexity':ppl})"
      ],
      "metadata": {
        "id": "xiQXCn3LePzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_generated_sequences = generated_sequences.sort_values(by = 'Perplexity') #Sort the amino acid sequences by their perplexity value; the lower the perplexity the value, the more confident we are in the output of the fine-tuned ProtGPT2 Model\n"
      ],
      "metadata": {
        "id": "canWEHHGjbxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following line of code is commented out because it writes the sorted_generated_sequences DataFrame to a csv file; there is no need to repeat this everytime the notebook is run."
      ],
      "metadata": {
        "id": "AFOSnaIoKJ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted_generated_sequences.to_csv('/content/gdrive/My Drive/team_5/project_2/GeneratedSequences.csv')"
      ],
      "metadata": {
        "id": "nrjZ7i2QKI5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PAM Prediction of Generated Cas9 Sequences Using Convolutional Neural Networks\n",
        "8 CNNs using the same model architecture were run: each time the VHSE-embedded protein inputs remained the same, but the 2D PAM labels were swapped out for each run of the model."
      ],
      "metadata": {
        "id": "ORWCvuswKa3I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_R0yyw3E46"
      },
      "source": [
        "###Training the CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twa0H-1nyXih"
      },
      "source": [
        "#### PAM Encoding for CNN\n",
        "This section contains the code by which PAM sequences were encoded in n x 4 matrices for use as a multi-label output in our 8 CNN models (1 model for each nucleotide position)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwviNC0JzN3l"
      },
      "source": [
        "The below dictionary assigns each of the base 4 nucleotides to a number; this is the column index number that will later be used when constructing the PAM encoded arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP0LiWA8Vqr9"
      },
      "outputs": [],
      "source": [
        "nt_dict = {'A':0,'C':1,'T':2,'G':3} #Create a matrix where each column corresponds to one of the 4 nucleotides in DNA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15PSAulZ1KcH"
      },
      "outputs": [],
      "source": [
        "#Create list for each nucleotide in PAM sequence\n",
        "nt1 = data_8nt['nt1'].tolist()\n",
        "nt2 = data_8nt['nt2'].tolist()\n",
        "nt3 = data_8nt['nt3'].tolist()\n",
        "nt4 = data_8nt['nt4'].tolist()\n",
        "nt5 = data_8nt['nt5'].tolist()\n",
        "nt6 = data_8nt['nt6'].tolist()\n",
        "nt7 = data_8nt['nt7'].tolist()\n",
        "nt8 = data_8nt['nt8'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollgBtbYxSdR"
      },
      "source": [
        "The dictionary below assigns all of the IUPAC ambiguity codes to their meanings in terms of the base 4 nucleotides. Thus, coupling this with the expand_PAM function defined below outputs a list with all of the possible DNA sequences only in terms of A,C,T,G even if the input sequence contains an ambiguity code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6nB1opKoSYc"
      },
      "outputs": [],
      "source": [
        "IUPAC_r = { #Create a dictionary for all of the ambiguous DNA codes, mapping them to their equivalent possibilites in terms of the 4 fundamnetal nucleotides\n",
        "    'N': '(A|C|T|G)',\n",
        "    'R': '(G|A)',\n",
        "    'Y': '(T|C)',\n",
        "    'M': '(A|C)',\n",
        "    'K': '(G|T)',\n",
        "    'S': '(G|C)',\n",
        "    'W': '(A|T)',\n",
        "    'H': '(A|C|T)',\n",
        "    'B': '(G|C|T)',\n",
        "    'V': '(A|C|G)',\n",
        "    'D': '(A|G|T)'\n",
        "}\n",
        "\n",
        "ambiguity_codes = list(IUPAC_r.keys()) #Create a list of the ambiguity codes\n",
        "\n",
        "#This function returns all of the possible PAMs if that PAM contains an ambiguity code\n",
        "#For example, NATG would return AATG, CATG, CATG, and TATG\n",
        "def expand_PAM(seq):\n",
        "  for s in seq:\n",
        "    if s in IUPAC_r:\n",
        "      seq = seq.replace(s, IUPAC_r[s])\n",
        "  seq = list(exrex.generate(seq))\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfGYcreCzh_p"
      },
      "source": [
        "The below function constructs the 2D PAM encoding array for each nucleotide position in the PAM sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYoxX0d3sMZZ"
      },
      "outputs": [],
      "source": [
        "def nt_encode(ntlist):\n",
        "  a = np.zeros((len(ntlist),4)) #Initialize an array that has number of rows equal to the number of samples passed into the function\n",
        "  for i in range(0,len(ntlist)):\n",
        "    if type(ntlist[i]) == str:\n",
        "      if ntlist[i] in ambiguity_codes:\n",
        "        e = expand_PAM(ntlist[i]) #Utilize the expand_PAM function if the nucleotide is an ambigiuity code\n",
        "        for x in e:\n",
        "         a[i][nt_dict[x]] = 1 #Place the value 1 in all of the relavent columns corresponding to the fundamental nucleotide possibilites\n",
        "      else:\n",
        "        a[i][nt_dict[ntlist[i]]] = 1 #If the nucleotide is not an ambiguity code, just place a 1 in the one column for that nucleotide\n",
        "\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYjYKSpPsPkW"
      },
      "outputs": [],
      "source": [
        "#Generate 2D PAM encoding arrays for each nucleotide position in the PAM sequence\n",
        "nt1_encoded = nt_encode(nt1)\n",
        "nt2_encoded = nt_encode(nt2)\n",
        "nt3_encoded = nt_encode(nt3)\n",
        "nt4_encoded = nt_encode(nt4)\n",
        "nt5_encoded = nt_encode(nt5)\n",
        "nt6_encoded = nt_encode(nt6)\n",
        "nt7_encoded = nt_encode(nt7)\n",
        "nt8_encoded = nt_encode(nt8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wIOpbNMDPyq"
      },
      "source": [
        "### VHSE Embedding of Protein Sequences from the original dataset\n",
        "This function takes in an amino acid sequence, splits the string into its component characters, and assembles a VHSE encoded matrix. Each row of the matrix represents one amino acid in the protein sequence, and there are 8 columns for each of the 8 VHSE values. The function also takes as an input the maximum length of a protein sequence so that adequate zero-padding can be applied to make sure all inputs are of the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkvmgVOlDO_Q"
      },
      "outputs": [],
      "source": [
        "#Using pandas to read in the csv file containing the VHSE encoding values for each amino acid\n",
        "VHSE_val = pd.read_csv(dataset_path + 'VHSE8.csv')\n",
        "\n",
        "def VHSE_encode(seq, max_len):\n",
        "    s = [*seq] #Split the amino acid sequence for each character in the string\n",
        "    m = []\n",
        "    for i in range(0,len(s)):\n",
        "        for l in range(0,np.shape(VHSE_val)[0]):\n",
        "            if s[i] == VHSE_val['Single Code'][l]:\n",
        "                m.append(VHSE_val.iloc[l][2:10]) #This adds all 8 VHSE values for a given amino acid\n",
        "    \n",
        "    if len(m) != max_len:\n",
        "       for x in range(0,max_len-len(m)):\n",
        "         m.append([0,0,0,0,0,0,0,0]) #This applies zero-padding so that the shape of all encoded protein sequences are the same. The amount of zero padding is based on the maximum protein sequence length\n",
        "\n",
        "    a = np.array(m).astype('float32')\n",
        "\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isWXG4vB2Pk5"
      },
      "source": [
        "The VHSE takes approximately 30 minutes to run due to the large volume of data. Thus, instead of having to re-embed the protein sequences every time we start a new runtime, the VHSE encoded protein sequences were saved as a csv file that could then be loaded and reshaped into the 3D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciJKJvoLSLUZ"
      },
      "outputs": [],
      "source": [
        "#VHSE_encoded = [VHSE_encode(seq,max_seq_len) for seq in protseqs]\n",
        "#VHSE_encoded = np.array(VHSE_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpYnGUgC2qKF"
      },
      "source": [
        "The two functions defined below allow for the export and import of the VHSE-embedded csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acVK80E3Anj6"
      },
      "outputs": [],
      "source": [
        "def export_matrix(a,file): #This function exports the 3D-dimensional VHSE array by first flattening it and writing it to a txt file\n",
        "    a_reshaped = a.reshape(a.shape[0],-1)\n",
        "    np.savetxt(file,a_reshaped)\n",
        "\n",
        "def import_matrix(file,twoshape): #This functions loads in the txt file and reshapes it back into the 3D numpy array\n",
        "  loaded_a = np.loadtxt(file)\n",
        "  load_original_a = loaded_a.reshape(loaded_a.shape[0],loaded_a.shape[1] // twoshape, twoshape)\n",
        "  return load_original_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B0792CP21PR"
      },
      "source": [
        "This code block has been commented because the csv export has already happened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XikkjwIMyw-"
      },
      "outputs": [],
      "source": [
        " #export_matrix(VHSE_encoded,'/content/drive/My Drive/team_5/project_1/Data/FinalData_VHSE_Encoded.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxfjkjIl8Ksn"
      },
      "source": [
        "The final embedding result is a 3D array, where each sample is encoded as a 1679 x 8 matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJBnBZKC4M-"
      },
      "outputs": [],
      "source": [
        "VHSE_encoded = import_matrix(dataset_path + 'FinalData_VHSE_Encoded.csv',8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbbDA64N30cH"
      },
      "source": [
        "####Reshaping the VHSE Protein Embedding\n",
        "The VHSE encoded array is reshaped so that it can be a tensor, which is the required input for the CNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzrQf7O3H4Y"
      },
      "outputs": [],
      "source": [
        "VHSE_encoded_tensor = np.expand_dims(VHSE_encoded,-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gccE3uQn43T-"
      },
      "source": [
        "####Splitting the Data for CNN\n",
        "The data for each nucleotide position was split as 72/8/20 for train/val/test. The VHSE-encoded protein sequences remain constant across all models, only the PAM nucleotide position labels are changing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzSxw3w37RnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eff047c-3203-4a19-a8fe-a262ce7863d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4508, 1697, 8, 1)\n",
            "(4508, 4)\n"
          ]
        }
      ],
      "source": [
        "train_seq, test_seq, train_nt1, test_nt1 = train_test_split(VHSE_encoded_tensor,nt1_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt1, val_nt1 = train_test_split(train_seq, train_nt1, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt2, test_nt2 = train_test_split(VHSE_encoded_tensor,nt2_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt2, val_nt2 = train_test_split(train_seq, train_nt2, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt3, test_nt3 = train_test_split(VHSE_encoded_tensor,nt3_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt3, val_nt3 = train_test_split(train_seq, train_nt3, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt4, test_nt4 = train_test_split(VHSE_encoded_tensor,nt4_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt4, val_nt4 = train_test_split(train_seq, train_nt4, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt5, test_nt5 = train_test_split(VHSE_encoded_tensor,nt5_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt5, val_nt5 = train_test_split(train_seq, train_nt5, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt6, test_nt6 = train_test_split(VHSE_encoded_tensor,nt6_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt6, val_nt6 = train_test_split(train_seq, train_nt6, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt7, test_nt7 = train_test_split(VHSE_encoded_tensor,nt7_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt7, val_nt7 = train_test_split(train_seq, train_nt7, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt8, test_nt8 = train_test_split(VHSE_encoded_tensor,nt8_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt8, val_nt8 = train_test_split(train_seq, train_nt8, test_size=0.1, random_state=42)\n",
        "\n",
        "print(np.shape(train_seq))\n",
        "print(np.shape(train_nt1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmLxQda558bh"
      },
      "source": [
        "###CNN Model Architecture and Training\n",
        "The CNN model architecutre consists of a convolution layer with 32 kernels of size (3,3), followed by a 2x2 maxpool layer, followed by another convolution layer with 64 kernels of size (3,3), followed by another 2x2 maxpool layer, followed by a flatten layer, dropout of 0.6, and a final dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJeXCETM7732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3aaa733-0a19-4338-9c57-e3124f40a105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1697, 8, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 849, 4, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 849, 4, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 425, 2, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 54400)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 54400)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 217604    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236,420\n",
            "Trainable params: 236,420\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = (1697,8,1)\n",
        "num_classes = 4\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    \n",
        "    [\n",
        "        keras.Input(shape = input_shape),\n",
        "        layers.Conv2D(32, kernel_size = (3,3), padding='same',activation=\"relu\"), #convolutional layer using 32 kernels of size (3,3) with the relu activation unit\n",
        "        layers.MaxPooling2D(pool_size = (2, 2),padding='same'), #Max pool layer of size (2,2)\n",
        "        layers.Conv2D(64, kernel_size = (3,3), padding='same',activation=\"relu\"),#convolutional layer using 32 kernels of size (3,3) with the relu activation unit\n",
        "        layers.MaxPooling2D(pool_size = (2, 2),padding='same'), #Max pool layer of size (2,2)\n",
        "        layers.Flatten(), #Flattens the output of the final maxpool layer\n",
        "        layers.Dropout(0.6), #Dropout of 0.6, which randomly drops 60% of nodes\n",
        "        layers.Dense(num_classes, activation=\"softmax\") #Dense layer in which the final classification predictions are made\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcUweA1e6DLF"
      },
      "source": [
        "A function was defined so that all 8 CNN models (1 for each nucleotide position) could be run with relative ease. The outputs display the loss and accuracy for the training and validation sets during training, and the test loss and accuracy during the model test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYVQPMqP9wVO"
      },
      "outputs": [],
      "source": [
        "def CNN(model,train_input,train_labels,val_input,val_labels,test_input,test_labels,name):\n",
        "  history = model.fit(x= train_input,\n",
        "                      y= train_labels,\n",
        "                      batch_size=60, \n",
        "                      callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)],\n",
        "                      validation_data = (val_input,val_labels),\n",
        "                      verbose = 1,\n",
        "                      epochs = 50)\n",
        "  \n",
        "  score = model.evaluate(x = test_input, \n",
        "                             y = test_labels,\n",
        "                             verbose=1)\n",
        "  \n",
        "#   model_json = model.to_json()\n",
        "#   with open(dataset_path+'VHSE_Encoded CNN Models/'+name+'.json', \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# # serialize weights to HDF5\n",
        "#     model.save_weights(dataset_path+'VHSE_Encoded CNN Models/'+name+'_weights'+\".h5\")\n",
        "#   print(\"Saved model to disk\")\n",
        "  model.save(dataset_path+'VHSE_Encoded CNN Models/'+name+'/')\n",
        "  \n",
        "  print('Test loss:', score[0]) \n",
        "  print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the 8 CNN models; 1 for each nucleotide"
      ],
      "metadata": {
        "id": "R0OrOHPfOYVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kIWZq7ecdWe"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt1,val_seq,val_nt1,test_seq,test_nt1,'Nucleotide1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2RSiEBNx3mP"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt2,val_seq,val_nt2,test_seq,test_nt2,'Nucleotide2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFFmJs7XyBNh"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt3,val_seq,val_nt3,test_seq,test_nt3,'Nucleotide3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oWFD_jRyCvD"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt4,val_seq,val_nt4,test_seq,test_nt4,'Nucleotide4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98lYCh5AyFSb"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt5,val_seq,val_nt5,test_seq,test_nt5,'Nucleotide5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WFFE_tRyKX9"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt6,val_seq,val_nt6,test_seq,test_nt6,'Nucleotide6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V9umOBVyNEg"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt7,val_seq,val_nt7,test_seq,test_nt7,'Nucleotide7')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT_IwWO4yQHs"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt8,val_seq,val_nt8,test_seq,test_nt8,'Nucleotide8')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predicting the PAMs of the ProtGPT2-generated Cas9 Sequences"
      ],
      "metadata": {
        "id": "Q1V-qe-7O26T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####VHSE Embedding of the generated Cas9 Sequences"
      ],
      "metadata": {
        "id": "c9DMbAkWPEpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to only move foward with sequences that had calculated perplexity scores less than 100. Thus, we are left with 18 de novo sequences that will be VHSE encoded and PAM-predicted with the trained CNN models."
      ],
      "metadata": {
        "id": "-wKAwI1tCxZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novel_seqs = pd.read_csv(dataset_path + 'GeneratedSequences.csv')\n",
        "novel_sequences = novel_seqs.query('Perplexity <= 100')['AA Sequence'].tolist() #Only those sequences with perplexity scores are kept"
      ],
      "metadata": {
        "id": "axAXkh3hPZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_VHSE_encoded = [VHSE_encode(seq,max_seq_len) for seq in novel_sequences] #VHSE encode the generated sequences with perplexity scores less than 100\n",
        "generated_VHSE_encoded = np.array(generated_VHSE_encoded)\n",
        "generated_VHSE_encoded_tensor = np.expand_dims(generated_VHSE_encoded,-1) #The VHSE encoded is expanded to a tensor so that it may be passed into the CNN models"
      ],
      "metadata": {
        "id": "KQE1RzahO2JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Predicting PAMs of the Generated Sequences"
      ],
      "metadata": {
        "id": "yEBt1AHGDU7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.serialization import DEFAULT_PROTOCOL\n",
        "# load a pre-trained model to predict on new data\n",
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "\n",
        "def predict_model(name):\n",
        "  prev_model = keras.models.load_model(dataset_path+'VHSE_Encoded CNN Models/'+name) #Loads the saved pre-trained CNN model\n",
        "  y_predict = prev_model.predict(generated_VHSE_encoded_tensor) #Runs the generated Cas9 sequences through the model for PAM prediction\n",
        "\n",
        "  df = pd.DataFrame(y_predict) #convert prediction matrix to dataframe\n",
        "  df.to_csv(dataset_path + 'Generated Sequence PAM Prediction/'+name+'.csv') #save prediction dataframe as csv\n",
        "  return y_predict\n"
      ],
      "metadata": {
        "id": "EhUqyM5uDUhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the PAM 1 nucleotide at a time for each of the 18 de novo Cas9 sequences, and export the prediction results to csvs\n",
        "nt1_predict = predict_model('Nucleotide1')\n",
        "nt2_predict = predict_model('Nucleotide2')\n",
        "nt3_predict = predict_model('Nucleotide3')\n",
        "nt4_predict = predict_model('Nucleotide4')\n",
        "nt5_predict = predict_model('Nucleotide5')\n",
        "nt6_predict = predict_model('Nucleotide6')\n",
        "nt7_predict = predict_model('Nucleotide7')\n",
        "nt8_predict = predict_model('Nucleotide8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoUk6PKuFSm1",
        "outputId": "a3e8cc0c-fa76-4bcd-ba9c-3b41e0222842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "OCRIwexSHqKC",
        "zsTjqxIFQKYi"
      ],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}